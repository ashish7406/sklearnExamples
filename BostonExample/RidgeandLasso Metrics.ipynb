{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import pandas as pd  \n",
    "import seaborn as sns \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading boston data\n",
    "from sklearn.datasets import load_boston\n",
    "boston_dataset = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "boston.head()\n",
    "names=boston.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
      "       'PTRATIO', 'B', 'LSTAT'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "boston['MEDV'] = boston_dataset.target\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(np.c_[boston['LSTAT'], boston['RM']], columns = ['LSTAT','RM'])\n",
    "Y = boston['MEDV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 2)\n",
      "(102, 2)\n",
      "(404,)\n",
      "(102,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=5)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "ls = Lasso(alpha=0.1) #a higher value of alpha restricts the coefficients further\n",
    "ls.fit(X_train,Y_train)\n",
    "Y_pred_train = ls.predict(X_train) #predictions on training data\n",
    "Y_pred = ls.predict(X_test) #predictions on testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lscoeficent=ls.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model performance for training set\n",
      "--------------------------------------\n",
      "RMSE is 5.639633479359648\n",
      "R2 score is 0.6297457828825357\n",
      "\n",
      "\n",
      "The model performance for testing set\n",
      "--------------------------------------\n",
      "MSE 26.80986506219231\n",
      "RMSE is 5.17782435605847\n",
      "R2 score is 0.657573887893668\n",
      "Difference between training and Testing set\n",
      "--------------------------------------\n",
      "Difference of RMSE between training and testing for Lasso is 0.46180912330117785\n"
     ]
    }
   ],
   "source": [
    "rmsetraining = (np.sqrt(mean_squared_error(Y_train, Y_pred_train)))\n",
    "\n",
    "r2 = r2_score(Y_train, Y_pred_train)\n",
    "\n",
    "print(\"The model performance for training set\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmsetraining))\n",
    "print('R2 score is {}'.format(r2))\n",
    "print(\"\\n\")\n",
    "\n",
    "# model evaluation for testing set\n",
    "y_test_predict = ls.predict(X_test)\n",
    "mse = mean_squared_error(Y_test,y_test_predict)\n",
    "rmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n",
    "r2 = r2_score(Y_test, y_test_predict)\n",
    "\n",
    "print(\"The model performance for testing set\")\n",
    "print(\"--------------------------------------\")\n",
    "print('MSE {}'.format(mse))\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))\n",
    "\n",
    "\n",
    "print(\"Difference between training and Testing set\")\n",
    "print(\"--------------------------------------\")\n",
    "print('Difference of RMSE between training and testing for Lasso is {}'.format(rmsetraining-rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "rr = Ridge(alpha=1) #a higher value of alpha restricts the coefficients further\n",
    "model=rr.fit(X_train,Y_train)\n",
    "Y_pred_train = rr.predict(X_train) #predictions on training data\n",
    "Y_pred = rr.predict(X_test) #predictions on testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.71920853,  4.55417009])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.98240259036276"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model performance for training set\n",
      "--------------------------------------\n",
      "RMSE is 5.637164235841213\n",
      "R2 score is 0.630069934330712\n",
      "\n",
      "\n",
      "The model performance for testing set\n",
      "--------------------------------------\n",
      "MSE 26.44419839700965\n",
      "RMSE is 5.142392283462012\n",
      "R2 score is 0.6622443259654345\n",
      "Difference between training and Testing set\n",
      "--------------------------------------\n",
      "Difference between training and Testing Dataset RMS Calculated for Ridge 0.49477195237920046\n"
     ]
    }
   ],
   "source": [
    "rmseRidgeTraining = (np.sqrt(mean_squared_error(Y_train, Y_pred_train)))\n",
    "r2 = r2_score(Y_train, Y_pred_train)\n",
    "\n",
    "print(\"The model performance for training set\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmseRidgeTraining))\n",
    "print('R2 score is {}'.format(r2))\n",
    "print(\"\\n\")\n",
    "\n",
    "# model evaluation for testing set\n",
    "y_test_predict = rr.predict(X_test)\n",
    "rmse = (np.sqrt(mean_squared_error(Y_test, y_test_predict)))\n",
    "mse = mean_squared_error(Y_test,y_test_predict)\n",
    "r2 = r2_score(Y_test, y_test_predict)\n",
    "\n",
    "print(\"The model performance for testing set\")\n",
    "print(\"--------------------------------------\")\n",
    "print('MSE {}'.format(mse))\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))\n",
    "\n",
    "print(\"Difference between training and Testing set\")\n",
    "print(\"--------------------------------------\")\n",
    "print(\"Difference between training and Testing Dataset RMS Calculated for Ridge {}\".format(rmseRidgeTraining-rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we observed that RMSE is lower in Ridge comparitive to Lasso and performed better. When we calculated  the  Difference between Training and Testing Data for RMS, it turned out that Lasso Performed better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
